{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c407db72",
      "metadata": {
        "id": "c407db72"
      },
      "source": [
        "\n",
        "# Laboratorium 6: Segmentacja 3D (MRI) — architektura typu V-Net\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eb3f4cf",
      "metadata": {
        "id": "7eb3f4cf"
      },
      "source": [
        "W tym ćwiczeniu również zajmiemy się problemem **segmentacji semantycznej** obszarów hipokampu, lecz tym razem dla danych trójwymiarowych za pomocą architektury typu **V-Net**.\n",
        "\n",
        "**Cele zajęć**\n",
        "- Wczytanie danych trójwymiarowych i sprawdzenie ich formatu.\n",
        "- Wykorzystanie **MONAI** do wczytywania i przetwarzania wstępnego danych 3D.\n",
        "- Trening sieci za pomocą losowych voxeli 3D.\n",
        "- Poznanie architektury typu **V-Net** oraz czym różni się od **U-Net**.\n",
        "- Wykonanie segmentacji danych trójwymiarowych oraz sprawdzenie skuteczności rozwiązania.\n",
        "\n",
        "**Dataset:**  \n",
        "Użyjemy **MSD Task04 Hippocampus** — zbioru MRI z adnotacjami hipokampa pochodzącego z Medical Segmentation Decathlon. Zawiera on około ~260 skanów z maskami (etykiety: 0 = tło, 1 = anterior hippocampus, 2 = posterior hippocampus). Dane zostaną automatycznie pobrane przez MONAI (`DecathlonDataset`). W notebooku dodatkowo przeprowadzamy: izotropizację do 1×1×1 mm, normalizację intensywności, oraz ekstrakcję 3D‑patchy z kontrolowanym stosunkiem próbek pozytywnych/negatywnych."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "739852c2",
      "metadata": {
        "id": "739852c2"
      },
      "source": [
        "## 0) Instalacja i importy\n",
        "\n",
        "Jeśli chcemy powtarzalnych wyników możemy ustawić stały SEED."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "38a4963e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38a4963e",
        "outputId": "dcdcfe49-e735-48ca-dec6-a7cb89c623ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | Lightning: 2.5.6\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -q install monai-weekly pytorch-lightning torchmetrics nibabel scikit-image --extra-index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "import os, tarfile, urllib.request, random, glob, math, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "# from torch.utils.data import DataLoader\n",
        "from monai.data import DataLoader\n",
        "\n",
        "\n",
        "from monai.apps import download_and_extract\n",
        "from monai.data import CacheDataset, Dataset, decollate_batch, list_data_collate\n",
        "from monai.transforms import (\n",
        "    Compose, LoadImaged, EnsureChannelFirstd, Orientationd, Spacingd,\n",
        "    NormalizeIntensityd, RandSpatialCropd, RandCropByPosNegLabeld,\n",
        "    RandFlipd, RandRotate90d, EnsureTyped, EnsureType, AsDiscrete,\n",
        "    CenterSpatialCropd\n",
        ")\n",
        "from monai.networks.nets import VNet\n",
        "from monai.losses import DiceCELoss\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.inferers import sliding_window_inference\n",
        "\n",
        "from monai.apps import DecathlonDataset\n",
        "from pathlib import Path\n",
        "from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, Orientationd, Spacingd, NormalizeIntensityd, RandCropByPosNegLabeld, RandFlipd, RandRotate90d, EnsureTyped, CenterSpatialCropd, SpatialPadd\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import LightningModule, LightningDataModule, Trainer\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from torchmetrics.classification import MulticlassJaccardIndex\n",
        "\n",
        "# SEED = 42\n",
        "SEED = 42\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s)\n",
        "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "\n",
        "if SEED is not None:\n",
        "    set_seed()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device:', device, '| Lightning:', pl.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9660c29",
      "metadata": {
        "id": "d9660c29"
      },
      "source": [
        "## Zadanie 1 – Konfiguracja parametrów, pobranie danych i przetwarzanie wstępne\n",
        "\n",
        "W tym zadaniu przygotujemy dane uczące i walidacyjne wykorzystując funkcje i transformacje z biblioteki MONAI. Parametry zostały już zdefiniowane w komórce z kodem poniżej. 4 pierwsze parametry są oczywiste, natomiast warto wyjaśnić znaczenie drugiej połowy.\n",
        "- `ROI_SIZE` - rozmiar wycinanych patchy 3D. Zamiast przetwarzać całą pojedynczą próbkę danych, dla trójwymiarowych częstym podejściem jest wycinanie z nich fragmentu (ROI) o zadanym rozmiarze i traktowanie tego ROI jako pojedynczej próbki.\n",
        "- `SAMPLES_PER_VOLUME` — liczba patchy pobieranych z jednego wolumenu uczącego dla każdej próbki w każdej epoce treningu. Zwiększenie wartości generuje więcej przykładów treningowych z każdego skanu (więcej wariancji), ale też zwiększa czas przetwarzania.\n",
        "- `POS_NEG_RATIO` — stosunek patchy wokół \"piksela\" pozytywnego (o masce większej od 0) do patchy wokół \"piksela\" negatywnego (należącego do tła) wybranych dla każdego przykładu treningowego. Przy małych obiektach (np. hipokamp) losowe ROI ma małe szanse trafić na obiekt, więc takie losowanie pozwala lepiej nauczyć model rozpoznawania struktur zamiast przeuczenia na tle.\n",
        "- `BATCH_SIZE` — liczba próbek w batchu podczas treningu (jak we wcześniejszych ćwiczeniach). Jednak tutaj ten parametr należy rozważać wspólnie z `SAMPLES_PER_VOLUME`. Najpierw wybierane jest `BATCH_SIZE` próbek ze zbioru danych, a następnie z każdej pobranej próbki pobierane jest `SAMPLES_PER_VOLUME` patchy o rozmiarze `ROI_SIZE`.\n",
        "\n",
        "Dlaczego stosujemy wycinanie patchy i opisane transformacje?\n",
        "- Efektywność pamięci i obliczeń: przetwarzanie pełnych wolumenów 3D jest zwykle bardzo kosztowne (pamięć GPU), dlatego uczymy na mniejszych patch'ach. Jest to szczególnie ważne dla zbiorów o dużych rozdzielczościach.\n",
        "- Zwiększenie liczby próbek: z jednego wolumenu możemy uzyskać wiele niezależnych patchy, co pomaga w trenowaniu (więcej różnych widoków/położeń struktury).\n",
        "- Radzenie sobie z nierównomiernym rozkładem klas: przez kontrolę pos/neg można wymusić, żeby model widywał wystarczająco dużo przykładów rzadkich klas.\n",
        "- Możliwość wykonywania obliczeń dla danych wejściowych o różnych rozmiarach.\n",
        "\n",
        "Należy również zauważyć, że wykorzystywany dataset ma tylko dwa podzbiory: treningowy i walidacyjny. Można ten problem rozwiązać na 2 sposoby. Pierwszym jest traktowanie zbioru walidacyjnego zarówno jako zbiór testowy. Drugim jest wydzielenie zbioru testowego ze zbioru walidacyjnego. W naszym rozwiązaniu wykorzystamy to pierwsze podejście.\n",
        "\n",
        "1. Zdefinujmy 3 listy transformacji: `pre_transforms`, `train_transforms` i `val_transforms`.\n",
        "2. `pre_transforms` jest listą, która zawiera początkowe przekształcenia, wykonywane są dla wszystkich wczytywanych danych:\n",
        "   - `LoadImaged(keys=[\"image\", \"label\"])` - wczytuje dane w formacie 'NIfTI' oraz odpowiadającą im maskę segmentacji,\n",
        "   - `EnsureChannelFirstd(keys=[\"image\", \"label\"])` - zapewnienie że kanał jest pierwszym wymiarem,\n",
        "   - `Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\")` - orientacja do standardowego układu RAS (Right-Anterior-Superior),\n",
        "   - `Spacingd(keys=[\"image\", \"label\"], pixdim=(1.0, 1.0, 1.0), mode=(\"bilinear\", \"nearest\"))` - izotropizacja voxeli do 1×1×1 mm, odpowiednio metodą biliniową dla danych uczących i metodą najbliższego sąsiada dla maski segmentacji,\n",
        "   - `NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True)` - normalizacja intensywności MRI\n",
        "3. Stwórz `train_transforms` za pomocą funkcji `Compose`, której argumentem jest lista transformacji. Na jej początku znajdować się powinny elementy z `pre_transforms`, a następnie dodatkowe augmentacje:\n",
        "   - `RandCropByPosNegLabeld` - losowe wycinanie patchy 3D z kontrolą pos/neg (parametry: `keys=[\"image\", \"label\"]`, `image_key=\"image`, `label_key=\"label\"`, `spatial_size` (rozmiar patch'y), `pos` (część patch'y wokół pikseli pozytywnych), `neg` (część patchy wokół pikseli negatywnych), `num_samples` (liczba patch'y dla każdej próbki)),\n",
        "   - `RandFlipd` x2 - losowe odbicie z zadanym prawdopodobieństwem `prob` dla osi `spatial_axis` 1 i 2,\n",
        "   - `EnsureTyped(keys=[\"image\", \"label\"])` - konwersja do tensorów.\n",
        "4. Stwórz `val_transforms` podobnie jak `train_transform` bez augmentacji. Do `pre_transforms` dodaj tylko:\n",
        "   - `CenterSpatialCropd(keys=[\"image\", \"label\"], roi_size=ROI_SIZE)` - wycinanie środkowego patcha. Dla walidacyjnego nie powinno być losowo wybieranych patchy, żeby osiągnąć spójne porównanie pomiędzy epokami.\n",
        "   - `EnsureTyped(keys=[\"image\", \"label\"])`.\n",
        "5. Pobierz i przygotuj zbiory używając `DecathlonDataset` (już w kodzie poniżej). Aby zmniejszyć wykorzystanie pamięci RAM możesz zmniejszyć parametr `cache_rate`. Oznacza on ilość danych, która po przetwarzaniu wstępnym zostanie zapamiętana w pamięci RAM.\n",
        "6. Wyświetl liczbę próbek w każdym zbiorze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "884c26d8",
      "metadata": {
        "id": "884c26d8"
      },
      "outputs": [],
      "source": [
        "# parametry\n",
        "TASK = \"Task04_Hippocampus\"  # MSD – Hippocampus\n",
        "DATA_ROOT = Path('./data/data_msd_decathlon')\n",
        "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "NUM_CLASSES = 3\n",
        "ROI_SIZE = (16, 16, 16)\n",
        "SAMPLES_PER_VOLUME = 16\n",
        "POS_NEG_RATIO = 0.5\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# automatyczne pobranie i przygotowanie zbiorów przez DecathlonDataset\n",
        "train_ds = DecathlonDataset(root_dir=DATA_ROOT, task=TASK, section=\"training\", transform=train_transforms, download=True, cache_rate=1.0)\n",
        "val_ds   = DecathlonDataset(root_dir=DATA_ROOT, task=TASK, section=\"validation\", transform=val_transforms, download=True, cache_rate=1.0)\n",
        "test_ds  = DecathlonDataset(root_dir=DATA_ROOT, task=TASK, section=\"validation\", transform=val_transforms, download=True, cache_rate=1.0)\n",
        "\n",
        "# Display shape of all the samples in the training dataset\n",
        "# for i in range(len(val_ds)):\n",
        "#     print(f\"Sample {i} - image shape: {val_ds[i]['image'].shape}, label shape: {val_ds[i]['label'].shape}\")\n",
        "\n",
        "print(f\"Zbiory: train={len(train_ds)}, val={len(val_ds)}, test={len(test_ds)}\")\n",
        "# RandCropByPosNegLabeld z num_samples>1 zwraca listę patchy, więc bierzemy pierwszy element\n",
        "# print(\"Przykład:\", train_ds[0][0][\"image\"].shape, train_ds[0][0][\"label\"].shape)\n",
        "\n",
        "# Wyświetlanie przykładowych próbek treningowych\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
        "for i in range(3):\n",
        "    sample = train_ds[i]\n",
        "    img = sample[0][\"image\"][0].cpu().numpy()  # [C, D, H, W] -> [D, H, W]\n",
        "    label = sample[0][\"label\"][0].cpu().numpy()\n",
        "\n",
        "    # Środkowy przekrój\n",
        "    mid_slice = img.shape[0] // 2\n",
        "\n",
        "    # Obraz\n",
        "    axes[0, i].imshow(img[mid_slice], cmap='gray')\n",
        "    axes[0, i].set_title(f'Próbka {i} - Obraz')\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "    # Maska\n",
        "    axes[1, i].imshow(label[mid_slice], vmin=0, vmax=NUM_CLASSES-1, cmap='jet')\n",
        "    axes[1, i].set_title(f'Próbka {i} - Maska')\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c5cf01",
      "metadata": {
        "id": "b1c5cf01"
      },
      "source": [
        "## Zadanie 2 – Implementacja LightningDataModule\n",
        "\n",
        "W tym zadaniu stworzymy klasę organizującą dostęp do danych zgodnie z konwencją PyTorch Lightning.\n",
        "\n",
        "1. Zaimplementuj klasę `HippoMonaiDataModule`, która dziedziczy po `pl.LightningDataModule`.\n",
        "2. Zaimplementuj metodę `__init__` z parametrami: `train_ds`, `val_ds`, `test_ds`, `batch_size`, `num_workers`.\n",
        "3. Wywołaj konstruktor klasy bazowej: `super().__init__()`.\n",
        "4. Zapamiętaj wszystkie argumenty.\n",
        "5. Zaimplementuj metodę `train_dataloader`, która zwraca `DataLoader` dla zbioru treningowego.\n",
        "6. Zaimplementuj metodę `val_dataloader` analogicznie, ale bez mieszania danych.\n",
        "7. `test_dataloader` w tym przypadku wskazuje na ten sam zbiór co `val_dataloader`.\n",
        "8. Stwórz instancję przygotowanej klasy.\n",
        "9. Pobierz jeden batch z train dataloadera: `b = next(iter(dm.train_dataloader()))`, a następnie sprawdź jego rozmiar. W tym celu wykorzystaj `b[\"image\"]` i `b[\"label\"]`.\n",
        "10. Zwróć uwagę na wielkość batch'a i czy zgadza się z przewidywaniami."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f52354b3",
      "metadata": {
        "id": "f52354b3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e6c81020",
      "metadata": {
        "id": "e6c81020"
      },
      "source": [
        "## Zadanie 3 – Implementacja V-Net\n",
        "\n",
        "W tym zadaniu stworzymy architekturę sieci typu V-Net oraz metody potrzebne do jej treningu.\n",
        "\n",
        "**Część A: Implementacja architektury**\n",
        "\n",
        "1. Zaimplementuj klasę `ConvBlock3d`, która dziedziczy po `nn.Module`:\n",
        "   - Konstruktor przyjmuje liczbę kanałów wejściowych i wyjściowych, rozmiar konwolucji, stride i padding.\n",
        "   - Definiuje: `self.conv` (`nn.Conv3d` bez bias), `self.bn` (`nn.BatchNorm3d`) i funkcję aktywacji (`nn.PReLU`). PReLU różni się od ReLU tym, że zamiast całkowicie zerować wartości ujemne, przepuszcza ich część z uczalnym współczynnikiem α, dzięki czemu neurony rzadziej „umierają”.\n",
        "   - Metoda `forward` przepuszcza dane wejściowe po kolei przez konwolucję, normalizację, a następnie funkcję aktywacji.\n",
        "\n",
        "2. Zaimplementuj klasę `UpConvBlock3d`, która dziedziczy po `nn.Module`:\n",
        "   - Jest ona symetryczna do klasy `ConvBlock3d`, lecz zamiast `nn.Conv3d` stosujemy `nn.ConvTranspose3d`.\n",
        "\n",
        "3. Zaimplementuj klasę `ResidualBlock3d`, która dziedziczy po `nn.Module`:\n",
        "   - Konstruktor przyjmuje liczbę kanałów (nie zmienia się ona w tym bloku) i liczbę bloków konwolucyjnych konwolucji.\n",
        "   - Stwórz listę kolejnych warstw w tym bloku. Pojedynczy element składa się z `nn.Conv3d` (rozmiar konwolucji 3, padding 1), `nn.BatchNorm3d` i `nn.PReLU`. Ich liczba zależna jest od przekazanego parametru.\n",
        "   - Połącz warstwy z listy za pomocą `nn.Sequential`.\n",
        "   - W metodzie `forward przepuść dane wejściowe przez zdefiniowany blok. Dodatkowo dodajemy połączenie rezydualne, czyli do wyjścia dodajemy wejście do sieci (nie zmieniamy rozmiaru przestrzennego i liczby warstw, więc rozmiary powinny się zgadzać).\n",
        "\n",
        "3. Zaimplementuj klasę `VNet3D`, która dziedziczy po `nn.Module`:\n",
        "   - Konstruktor przyjmuje liczbę kanałów wejściowych, liczbę klas i bazową liczbę kanałów w kolejnych warstwach.\n",
        "   - **Enkoder:**\n",
        "     - Składa się on z na zmianę połączonych warstw `ConvBlock3d` i `ResidualBlock3d`. Pierwsza para nie zmniejsza rozdzielczości przestrzennej, tylko zwiększa liczbę kanałów do bazowej.\n",
        "     - W kolejnych parach rozdzielczość przestrzenna jest zmniejszana przez zastosowanie stride równego 2 w `ConvBlock3d`, natomiast liczba kanałów zwiększana jest dwukrotnie.\n",
        "     - W kolejnych parach rośnie również liczba bloków w warstwach `ResidualBlock3d`.\n",
        "     - Stosujemy konwolucje o rozmiarze 3 i padding'u równym 1.\n",
        "   - **Dekoder:**\n",
        "     - Składa się on z na zmianę połączonych warstw `UpConvBlock3d` i `ResidualBlock3d`.\n",
        "     - W kolejnych parach rozdzielczość przestrzenna jest zwiększana, natomiast liczba kanałów jest zmniejszana.\n",
        "     - Zmniejsza się również liczba bloków w warstwach `ResidualBlock3d` dla kolejnych par.\n",
        "     - W blokach `UpConvBlock3d` stosujemy konwolucje o rozmiarze 2, stride równym 2, a padding równym 0.\n",
        "     - Na koniec liczbę kanałów zmniejszamy do liczby klas za pomocą `nn.Conv3d` o rozmiarze 1.\n",
        "   - Metoda `forward`:\n",
        "     - Najpierw przepuszczamy dane wejściowe przez kolejne warstwy enkodera.\n",
        "     - Dla dekodera musimy dodać również połączenia z enkodera.\n",
        "     - Zamiasat konkatenacji kanałów w V-Net stosujemy dodawanie.\n",
        "     - Dla wyjścia z każdego bloku `UpConvBlock3d` dodajemy wyjście z odpowiadającego bloku `ConvBlock3d` w enkoderze.\n",
        "\n",
        "**Część B: Implementacja treningu**\n",
        "\n",
        "1. Zaimplementuj klasę `LitVNet`, która dziedziczy po `LightningModule`:\n",
        "   - Konstruktor przyjmuje liczbę kanałów wejściowych, liczbę klas i learning rate.\n",
        "   - Wywołaj `super().__init__()` oraz `self.save_hyperparameters()`\n",
        "   - Stwórz instancję zaimplementowanej wcześniej sieci.\n",
        "   - Zdefiniuj funkcję straty `DiceCELoss(to_onehot_y=True, softmax=True, squared_pred=False, include_background=True, lambda_dice=1.0, lambda_ce=1.0)`\n",
        "   - Zdefiniuj metryki:\n",
        "     - Dice: `DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)`\n",
        "     - IoU: `MulticlassJaccardIndex(num_classes=out_channels, average=\"macro\")`\n",
        "\n",
        "2. Zaimplementuj metodę `forward(self, x)` zwracającą wyjście sieci.\n",
        "\n",
        "6. Zaimplementuj metodę `configure_optimizers`:\n",
        "   - Pptymalizator Adam: `torch.optim.Adam(self.parameters(), lr=self.hparams.lr)`\n",
        "   - Scheduler cosinusowy: `torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)`\n",
        "   - Zwróć słownik: `{\"optimizer\": opt, \"lr_scheduler\": sch}`\n",
        "\n",
        "7. Zaimplementuj metody `training_step`, `validation_step` i `test_step`:\n",
        "   - Pobierz dane: `batch[\"image\"]` `batch[\"label\"].long()`.\n",
        "   - Oblicz predykcje: `self(x)`.\n",
        "   - Oblicz stratę.\n",
        "   - W bloku `with torch.no_grad():` oblicz metryki:\n",
        "     - `preds = torch.argmax(logits, dim=1)`\n",
        "     - Konwersja do one-hot dla metryki Dice:\n",
        "       - `y_oh = [AsDiscrete(to_onehot=self.hparams.out_channels)(yy) for yy in decollate_batch(y)]`\n",
        "       - `p_oh = [AsDiscrete(argmax=True, to_onehot=self.hparams.out_channels)(pp) for pp in decollate_batch(logits)]`\n",
        "     - `decollate_batch` dzieli batch na listę pojedynczych próbek, co jest wymagane przez niektóre metryki MONAI\n",
        "     - Oblicz Dice: `self.dice_metric(y_pred=p_oh, y=y_oh)`; `dice = self.dice_metric.aggregate().item()`; `self.dice_metric.reset()`\n",
        "     - Oblicz mIoU: `miou = self.miou_metric(preds, y.squeeze(1))`\n",
        "   - Zaloguj metryki:\n",
        "     - `self.log(f\"{stage}_loss\", loss, on_epoch=True, prog_bar=True)`\n",
        "     - `self.log(f\"{stage}_dice\", dice, on_epoch=True, prog_bar=(stage!='train'))`\n",
        "     - `self.log(f\"{stage}_miou\", miou, on_epoch=True, prog_bar=(stage!='train'))`\n",
        "   - Zwróć `loss` dla metody `training_step`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfc91c20",
      "metadata": {
        "id": "cfc91c20"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "cb8a61a3",
      "metadata": {
        "id": "cb8a61a3"
      },
      "source": [
        "## Zadanie 4 – Trening\n",
        "\n",
        "1. Stwórz instancję modelu.\n",
        "2. Skonfiguruj callback `EarlyStopping`.\n",
        "3. Skonfiguruj callback `ModelCheckpoint`.\n",
        "4. Stwórz `Trainer`. Przekaż liczbę epok, callbacki i częstotliwość logowania `log_every_n_steps=5`.\n",
        "5. Uruchom trening metodą `fit`.\n",
        "6. Wczytaj najlepszy model metodą `load_from_checkpoint`.\n",
        "7. Uruchom ewaluację na zbiorze testowym: `trainer.test(best, datamodule=dm)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8301c10",
      "metadata": {
        "id": "c8301c10"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0fff8cc8",
      "metadata": {
        "id": "0fff8cc8"
      },
      "source": [
        "## Zadanie 5 – Inferencja na pełnym wolumenie i wizualizacja\n",
        "\n",
        "W tym zadaniu wykonamy predykcję na pełnym wolumenie MRI i zwizualizujemy wyniki.\n",
        "\n",
        "1. Przełącz wczytany model w tryb ewaluacji i przenieś na odpowiednie urządzenie.\n",
        "2. Pobierz pierwszy przypadek ze zbioru testowego: `case = dm.test_ds[0]`.\n",
        "3. Przygotuj wolumen do inferencji:\n",
        "   - Pobierz obraz: `case[\"image\"]`\n",
        "   - Dodaj wymiar batch: `.unsqueeze(0)`\n",
        "   - Przenieś na urządzenie modelu: `.to(best.device)`\n",
        "4. Pobierz ground truth: `gt = case[\"label\"].squeeze(0).cpu().numpy()`.\n",
        "5. W bloku `with torch.no_grad():` wykonaj inferencję:\n",
        "   - Użyj `sliding_window_inference` z parametrami:\n",
        "     - `vol` - wejściowy wolumen,\n",
        "     - `roi_size=ROI_SIZE` - rozmiar okna przesuwnego,\n",
        "     - `sw_batch_size=BATCH_SIZE` - liczba okien przetwarzanych jednocześnie,\n",
        "     - `predictor=best` - model do predykcji,\n",
        "     - `overlap=0.5` - nakładanie się okien (50%).\n",
        "     - Funkcja `sliding_window_inference` dzieli wolumen na nakładające się okna o zadanym rozmiarze i przetwarza je partiami przez podany model. Predykcje z poszczególnych okien są następnie uśredniane, aby otrzymać spójną predykcję dla całego wolumenu.\n",
        "6. Konwertuj logity na predykcje klas: `pred = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()`.\n",
        "8. Stwórz wizualizację z trzema środkowymi przekrojami:\n",
        "   - Oryginalny obraz MRI,\n",
        "   - Maska (GT),\n",
        "   - Predykcja."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce246e65",
      "metadata": {
        "id": "ce246e65"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e6c93281",
      "metadata": {
        "id": "e6c93281"
      },
      "source": [
        "---\n",
        "\n",
        "## Podsumowanie i wnioski\n",
        "\n",
        "Odpowiedz krótko na poniższe pytania.\n",
        "\n",
        "1. Co oznacza ROI_SIZE w przypadku segmentacji 3D jak w wykonanym ćwiczeniu?\n",
        "2. Wyjaśnij parametry SAMPLES_PER_VOLUME i POS_NEG_RATIO.\n",
        "3. W jaki sposób V-Net różni się od U-Net (np. sposób łączenia ścieżek enkodera/dekodera)?\n",
        "4. Jak działa sliding_window_inference i jaki wpływ ma parametr overlap na jakość predykcji?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}